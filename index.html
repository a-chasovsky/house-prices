<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>TITLE</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<style>
    body {
        background-color: #FFFFFF
    }
    table 
        {
        width:100%;
    }
    td {
        padding-top:5px;
        padding-right:25px;
        padding-left:25px
    }
    table tr td:last-of-type {
        border: none;
    }
    /* .header {
        height: 165px;
        background-image: url(img/header.png);
        background-size: contain;
        background-repeat: no-repeat;
      } */
    .pic-width-adjust {
        height: 165px;
        background-size: contain;
        background-repeat: no-repeat;
      }
    pic-width-adjust1 {
          left: 0;
          right: 0;
          
          width: 100%;
          height: auto;
          
          object-position: center;
        }
    pic-center {
        display: flex;
        justify-content: center;
    }
    pic-margin {
        margin-left: 50px;
    }
    h1 {
        font-family: Ubuntu, system-ui;
        color: #26292D;
        font-size: 1.5em;
        font-weight: 600;
        text-align: left;
        line-height: 1.05;
        margin-bottom: 0px;
    }
    h2 {
        font-family: Ubuntu, system-ui;
        color: #26292D;
        font-size: 1.3em;
        font-weight: 600;
        text-align: left;
        line-height: 1.05;
        margin-bottom: 0px;
    }
    h3 {
        font-family: Ubuntu, system-ui;
        color: #26292D;
        font-size: 1.15em;
        font-weight: 600;
        text-align: left;
        line-height: 1.05;
        margin-bottom: 0px;
    }
    h4 {
        font-family: Ubuntu, system-ui;
        color: #26292D;
        font-size: 1em;
        font-weight: 600;
        text-align: left;
        line-height: 1.05;
        margin-bottom: 0px;
    }
    p {
        font-family: system-ui;
        line-height: 1.5;
        margin: 12.5px 0;
        text-align: justify;
        }
    #toc_container {
        background: #FFFDFF;
        border: 1px solid #BBBBBB;
        display: table;
        font-size: 95%;
        margin-bottom: 1em;
        padding: 20px;
        width: auto;
    }
    .toc_title {
        font-weight: 700;
        text-align: left;
        font-family: 'Ubuntu';
        color: #23292F;
    }
    text-number-toc {
        font-family: system-ui;
        font-weight:600;
        color: #333333;
        font-size: 0.85em;
        text-align: justify;
        line-height: 1.5;
    }    
    a:link {
        font-family: 'system-ui';
        text-decoration: none;
        color: #164B84;
    }
    a:visited {
        font-family: 'system-ui';
        color: #164B84;
    }
    a:hover {
        font-family: 'system-ui';
        color: #164B84;
        text-decoration: underline;
    }
    a:active {
        font-family: 'system-ui';
          color: #1071C1;
    }
    sb {
        font-weight:500;
    }
    line-height-1-75 {
        line-height: 1.75em;
    }
    space {
        display: block;
        margin-top: 12.5px;
        content: ' '
    }
    ul {
        list-style-type: disc;
        line-height: 1.5;
        margin-top: -5px;
    }
    li {
        font-family: system-ui;
        line-height: 1.75em;
    }
    /* t {
        font-family: system-ui;
        color: #333333;
        font-size: 1em;
        text-align: justify;
        line-height: 1.5;
        margin-bottom: 1em;
    } */
    
</style>

<body>

<table>
    <tr>
        <td style='width:20%;border:none'></td>
        <td style='width:65%;border:none;padding-bottom:15px'>
            <div class='pic-width-adjust' style='background-image:url(img/header.png);'></div>
            <h1>
                Комбинирование регрессионнного анализа и машинного обучения для более точных прогнозов и высокой интерпретируемости модели
                </h1>
            <p>
               <i style='font-size:0.85em'>Регрессионный анализ, машинное обучение, описательная аналитика, предиктивная аналитика</i> 
                </p>
            </td>
        <td style='width:55%;border:none'></td>
        </tr>
    <tr>
        <td style='width:20%;border:none'></td>
        <td style='width:65%;border-right:none;border-left:1px solid #BBBBBB'>
            <div id="toc_container">
                <div class='toc_title'>
                    <p>Содержание</p>
                    </div>
                 <line-height-1-75>
                    <text-number-toc>1.</text-number-toc> <a href="#review"> Обзор проекта</a><br>
                    <text-number-toc>2.</text-number-toc> <a href="#dataset_description"> Описание датасета</a><br>
                    <text-number-toc>3.</text-number-toc> <a href="#missed_values">Работа с отсутствующими значениями</a><br>
                    <text-number-toc>4.</text-number-toc> <a href="#model_base">ML-модель №1 (базовая)</a><br>
                    <text-number-toc>5.</text-number-toc> <a href="#eda">Исследование данных</a><br>
                    <text-number-toc>6.</text-number-toc> <a href="#clear">Очистка данных</a><br>
                    &emsp;<text-number-toc>6.1.</text-number-toc> <a href="#discrepancy">Проверка данных на несоответствие</a><br>
                    &emsp;<text-number-toc>6.2.</text-number-toc> <a href="#outliers">Выявление и удаление выбросов</a><br>
                    <text-number-toc>7.</text-number-toc> <a href="#model_inter1">ML-модель №2 (после очистки данных)</a><br>
                    <text-number-toc>8.</text-number-toc> <a href="#feature_engineering">Создание дополнительных предикторов</a><br>
                    &emsp;<text-number-toc>8.1.</text-number-toc> <a href="#square_feet">Предикторы, характеризующие площадь</a><br>
                    &emsp;<text-number-toc>8.2.</text-number-toc> <a href="#rooms">Предикторы, характеризующие помещения (жилые комнаты, ванные комнаты, кухни)</a><br>
                    &emsp;<text-number-toc>8.3.</text-number-toc> <a href="#dates">Предикторы, характеризующие даты</a><br>
                    <text-number-toc>9.</text-number-toc> <a href="#preprocessing">Предварительная подготовка данных</a><br>
                    <text-number-toc>10.</text-number-toc> <a href="#model_inter2">ML-модель №3 (после создания новых признаков)</a><br>
                    <text-number-toc>11.</text-number-toc> <a href="#model_linear">Линейные модели и регрессионный анализ</a><br>
                    &emsp;<text-number-toc>11.1.</text-number-toc> <a href="#elastic_net">Эластичная сеть</a><br>
                    &emsp;<text-number-toc>11.2.</text-number-toc> <a href="#linear_regression">Линейная регрессия</a><br>
                    &emsp;<text-number-toc>11.3.</text-number-toc> <a href="#simulations">Результаты линейных моделей на сэмплированных данных</a><br>
                    <text-number-toc>12.</text-number-toc> <a href="#residuals">Выбор ML-модели для прогнозирования остатков линейной регрессии</a><br>
                    &emsp;<text-number-toc>12.1</text-number-toc> <a href="#ml_simple">Опорные вектора, бустинги, бэггинг, случайный лес, K-ближайших соседей</a><br>
                    &emsp;<text-number-toc>12.2.</text-number-toc> <a href="#vote_stack">Добавление оценщиков верхнего уровня - Voting и Stacking</a><br>
                    &emsp;<text-number-toc>12.3.</text-number-toc> <a href="#residuals_final">Сравнение всех моделeй и выбор финального оценщика остатков регрессии</a><br>
                    &emsp;<text-number-toc>12.4.</text-number-toc> <a href="#hpp">Комбинирование линейной регрессии и ML-алгоритма - итоговый оценщик HousePricePredictor</a><br>
                    <text-number-toc>13.</text-number-toc> <a href="#ml_models">Оценка независимых ML-моделей</a><br>
                    <text-number-toc>14.</text-number-toc> <a href="#final_results">Подведение итогов</a><br>
                    &emsp;<text-number-toc>14.1.</text-number-toc> <a href="#all_models">Сравнение всех моделей</a><br>
                    &emsp;<text-number-toc>14.2.</text-number-toc> <a href="#predicted_actual">Актуальные и спрогнозированные цены</a><br>
                    &emsp;<text-number-toc>14.3.</text-number-toc> <a href="#hpp_residuals">Анализ остатков HPP</a><br>
                    &emsp;<text-number-toc>14.4.</text-number-toc> <a href="#further">Дальнейшие шаги</a><br>
                    </line-height-1-75>
                </div>
            </td>
        <td style='width:55%;border:none'></td>
        </tr>
    <tr>
        <td style='width:20%;border:none'></td>
        <td style='width:65%;border-right:1px solid #CFD7DE;border-left:1px solid #BBBBBB'>
            <h2>
            <a
                class='anchor'
                id='review'>
                </a>
            1. Обзор проекта и краткие результаты
            </h2>
            <h3>
                Цель
                </h3>
            <p>
            Современные алгоритмы машинного обучения обычно превосходят классические линейные модели в точности прогнозов, но при этом сложны в интерпретации. Некоторые алгоритмы имеют встроенный функционал приоритезации предикторов, что позволяет выявить наиболее информативные. Тем не менее, этого недостаточно для оценки взаимосвязей.<br>
            <space></space>
            Целью данного проекта является создание простой в интерпретации модели <sb>HousePricePredictor</sb>, которая по точности прогнозов не уступает лучшим алгоритмами машинного обучения. Модель состоит из двух уровней: на первом уровне используется линейная регрессия для выдачи первичных прогнозов и определения коэффициентов для интерпретации; второй уровень - это алгоритм машинного обучения, прогнозирующий величину остатков (ошибок) регрессии для увеличения тчоности прогнозов. Эта схема во многом напоминает первую итерацию бустинга, в котором каждая последующая итерация минимизирует ошибку предыдущей.
                </p>
            <h3>
                Этапы
                </h3>
            <h4>
                <i>Секция 1: Исследование и подготовка данных</i>
                </h4>
            <p>
                На первом этапе проводятся необохдимые процедуры анализа и подготовки данных: заполняются пропущенные значения, выявляются и исправляются противоречия в данных, определяются наиболее значимые предикторы, создаются дополнительные предикторы, осуществляются необходимые преобразования. На каждом этапе преобразования данных обучается промежуточная ML-модель LightGBM.
                <space></space>
                <i>Результаты:</i>
                </p>
            <ul>
                <li>очищенный и преобразованный датасет;</li>
                <li>датасеты для пайплайнов секции машинного обучения;</li>
                <li>три промежуточные ML-модели.</li>
            </ul>
            <h4>
                <i>Секция 2: Регрессионный анализ</i>
                </h4>
            <p>
                В данном разделе проводится исследование эластичной сети и линейной регрессии без регуляризации. Сначала строится эластичная сеть, подбираются оптимальные гиперпараметры и оцениваются разные наборы предикторов. Далее десять наиболее релевантных претикторов используются для обучения простой линейной регрессии без регуляризации, которая и будет использоваться на первом уровне итогового оценшика HPP. В заключении проводится диагностика получившейся регерссии и сравнение её с эластичной сетью на сэмплированных данных.
            <space></space>
            <i>Результаты:</i>
                </p>
            <ul>
                <li>линейная модель первого уровня итогового оценщика HPP</li>
                <li>эластичная сеть для сравнения с другими моделями.</li>
            </ul>
            <h4>
                <i>Секция 3: Машинное обучение</i>
                </h4>
            <p>
                Секция машинного обучения состоит из двух частей.
                <space></space>
                В первой части осуществляется выбор алгоритма, который будет прогнозировать <sb>остатки</sb> (ошибки) линейной регрессии. Его прогнозы в дальнейшем используются для коррекции прогнозов регрессии, что теоретически должно привести к улучшению точности предсказания цены. Для оценки лучшего алгоритма используется 20-фолдовая перекрестная проверка.
                <space></space>
                Во второй части секции по той же схеме с 20-фолдовой кросс-валидацией выбирается лучшая модель машинного обуения для прогноза <sb>цены</sb> домов. Эта модель - очередной бенчмарк эффективности (в дополнение к моделям из предыдущих секций) для итогового оценщика HPP.
                <space></space>
                <i>Результаты:</i>
                </p>
            <ul>
                <li>ML-модель для прогноза остатков регрессии;</li>
                <li>ML-модель для прогноза цены дома.</li>
                </ul>
            <h4>
                <i>Секция 4: Подведение итогов</i>
                </h4>
            <p>
                На заключительном этапе проводится сравнение имеющихся моделей, делается заключение об эффективности оценщика HPP, исследуются остатки его прогнозов, предлагаются шаги для дальнейшего исследования данных и развития структуры HPP.
            <space></space>
            <i>Результаты:</i>
                </p>
            <ul>
                <li>оценка HPP;</li>
                <li>рекомендации по дальнейшим шагам.</li>
            </ul>
            <h3>
                Результаты
                </h3>
            <h4>
                <i>Предсказательная эффективность HousePricePredictor (HPP)</i>
                </h4>
            <p>
                График показывает эффективность прогнозирования семи моделей, построенных на разных стадиях проекта.
                </p>
                <ul>
                    <li>Base - базовая модель LightGBM после удаления пропусков;</li>
                    <li>Base (Clean) - промежуточная модель LightGBM после очистки данных;</li>
                    <li>Base (FE) - промежуточная модель LightGBM после генерации новых признаков;</li>
                    <li>Elastic Net - лучшая регрессионная модель (эластичная сеть);</li>
                    <li>SVM - лучшая линейная модель;</li>
                    <li>Stacking - лучшая ML-модель;</li>
                    <li>HousePricePredictor(HPP) - модель, полученная в результате комбинации линейной регрессии и стэкинга.</li>
                </ul>
                <space></space>
                <img src='img/Section8-Final-predictions/final_plot.png'>
                <space></space>
            <p>
                В ходе проекта удалось снизить ошибку RMSE c 0.1216 у базовой модели до 0.0969 итогового оценщика HousePricePredictor. На этапе преобразования наибольший вклад внесла очистка данных, а дополнительно сгенерированные предикторы были полезны как для линейной регрессии, так и для сложных алгоритмов машинного обучения (см. <i>'8. Создание дополнительных предикторов'</i>, <i>'11.2. Линейная регрессия'</i>, <i>'13. Оценка независимых ML-моделей'</i>).
                <space></space>
                Результат итогового оценщика HPP превысил ожидания - это единственная модель, у которой ошибка на тестовых данных меньше 0.1. Доверительные интервалы средней ошибки и предиктивные интервалы у всех моделей примерно одинаковы.
                <space></space>
                Три лучшие модели демонстрируют неплохой разброс - их прогнозы достаточно кучны, за исключением нескольких выбросов, хорошо заметных на нижнем графике. Для HPP это пять элементов с заметно более высоким значением ошибки.
                </p>
            <h4>
                <i>Интерпретациия модели</i>
                </h4>
            <p>
                Ядром HousePricePredictor является линейная регрессия. Коэффициенты и доверительные интервалы представлены в таблице ниже.
                <space></space>
                <pic-margin><img src='img/Section4-Linear-models-research/coefficients_table.png'></pic-margin>
                <space></space>
                Предикторы с наибольшим коэффициентом - это дополнительно сгенерированный предиктор <sb>lg_flrsfmean</sb> (средневзвешенная площадь первого и второго этажей, см. <i>'8.1. Предикторы, характеризующие площадь'</i>), общее качество строительных и отделочных материалов <sb>overallqual</sb>, возраст дома <sb>houseage</sb>. Вклад, котоый вносит увеличение этих параметров в изменение цены дома (при прочих равных):
                </p>
                <ul>
                    <li>С увеличением средневзвешенной площади этажей на 1% цена увеличивается на 0.144%;</li>
                    <li>С увеличением качества материалов на 1 пункт цена увеличивается на 8.8%;</li>
                    <li>С увеличением возраста дома на 1 год цена уменьшается на 8.5%</li>
                </ul>
            <p>
                Интерпретацию остальных коэффцииентов см. в разделе <i>'11.2. Линейная регрессия'.</i>
                </p>
            <h4>
                <i>Сравнение актуальных и предсказанных HousePricePredictor цен</i>
                </h4>
                <space></space>
                <img src='img/Section8-Final-predictions/predicted_actual_residuals.svg'>
                <space></space>
            <p>
                На оси абсцисс - элементы, отсортированные по суммарной площади жилого пространства <sb>grlivarea</sb>. С увеличением площади увеличивается цена дома. Этот график можно разбить на три составляющие: малые, средние и большие дома.
                <space></space>
                В первой части для небольших домов значения ошибок невелики, и модель чаще переоценивает дома. На среднем отрезке ошибки увеличиваются, предсказания модели выравниваются. На финальном отрезке для больших домов вновь чаще наблюдается переоценка, а ошибки в абсолютных величинах иногда превышают 50000$.
            </p>
            <h2>
                <a
                    class='anchor'
                    id='dataset_description'>
                    </a>
                2. Описание датасета
                </h2>
            <p>
                В проекте используется один из датасетов Kaggle. Это типичная задача регрессии, в которой требуется спрогнозировать стоимость дома, используя для этого значения его характеристик и исторические данные о предыдущих сделках.
                <space></space>
            <a
                href='https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview'
                >Перейти к странице датасета на сайте Kaggle
                </a>
                <space></space>
                Число предикторов: 79;<br>
                Число элементов: 1460;
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/data_raw.png'>
                <space></space>
                После разделения датасета на обучающий (train) и проверочный (test), число элементов составило 1314 и 146 соответственно.
                <space></space>
                </p>
            <h4>
                <i>Целевая переменная</i>
                </h4>  
            <p>
                Распределение целевой переменной напоминает гамма-распределение. Это одномодальное распределение с вершиной в районе 150000$ и правосторонней ассиметрией.
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/target_distribution.png'>
                <space></space>
                Для выравнивания формы распределения и снижения влияния экстремальных значений используется натуральный логарифм.
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/target_distribution_log.png'>
                <space></space>
                </p>
                <h4>
                    <i>Характеристики</i>
                    </h4>
            <p>
                В датасете 79 независимых переменных:
                </p>
                <ul>
                    <li>Количественных признаки - 34;</li>
                    <li>Порядковые признаки - 18;</li>
                    <li>Категориальные признаки - 27.</li>
                </ul>
            <h4>
                <i>Группы предикторов</i>
                </h4>
            <p>
                Для более детального анализа в ходе работы предикторы разделяются на группы, соответствующие общей логике. Этот подход полезен для генерации дополнительных признаков, являющихся производными от основных характеристик, а также в овремя очистки и проверки данных на логические противоречия.
                <space></space>
                Всего сформировано 12 категорий предикторов:
                <ul>
                    <li>Общая оценка качества</li>
                    <li>Гараж</li>
                    <li>Подвал</li>
                    <li>Предикторы, связанные с площадью</li>
                    <li>Комнаты</li>
                    <li>Год постройки дома/гаража, модернизации, и т.д.</li>
                    <li>Двор и область вокруг дома</li>
                    <li>Строительный материал</li>
                    <li>Конструкционные особенности</li>
                    <li>Состояние (износ)</li>
                    <li>Удобства</li>
                    <li>Окружение (район, улица, и т.д.)</li>
                    <li>Организационные аспекты продажи</li>
                </ul>
                </p>
            <h2>
                <a
                    class='anchor'
                    id='missed_values'>
                    </a>
                3. Работа с отсутствующими значениями
                </h2>
            <h4>
                <i>Категориальные переменные</i>
                </h4>   
            <p>
                На первый взгляд у некоторых категориальных переменных число пропущенных значений крайне велико и достигает почти 100% от общего числа элементов.
                <space></space>
                <i>Порядковые</i>
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/categorical_ordinal.png'>
                <space></space>
                <i>Номинативные</i>
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/categorical_categorical.png'>
                <space></space>
                <i>Заполнение пропусков</i>
                <space></space>
                Большое количество пропусков объясняется тем, что большинство NaN в этом датасете эквивалентны значению "Отсутствует" (к примеру, NaN для переменной "бассейн" означает, что в этом доме отсутствует бассейн). После приведения данных в соответствие с описанием и замены NaNs на 'NA' осталось только одно пропущенное значение в переменной <sb>electrical</sb> (заполняется медианным значением).
            </p>
            <h4>
                <i>Количественные переменные</i>
                </h4> 
            <p>            
                У трех количественных переменных присутствуют пропущенные значения: <sb>lotfrontage</sb>, <sb>garageyrblt</sb>, <sb>masvnrarea</sb>.
                </p>
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/quantitative.png'>
                <space></space>
            <ul>
                <li><sb>lotfrontage</sb> - длина участка улицы, соприкасающейся с придомовой территорией</li>
            </ul>
            <p>
                &emsp;&emsp;&ensp;237 (18%) пропущенных значений, пропуски заполняются медианным значением. Дополнительно обнаружены два потенциальных выброса (выделены красным).
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/lotfrontage_outs.png'>
                <space></space>
                </p>
            <ul>
                <li><sb>garageyrblt</sb> - год постройки гаража</li>
            </ul>
            <p>
                &emsp;&emsp;&ensp;73 (6%) пропущенных значений. Пропуски заполняются годом постройки дома.
                </p>
            <ul>
                <li><sb>masvnrarea</sb> - площадь облицовки фундамента</li>
            </ul>
            <p>
                &emsp;&emsp;&ensp;8 (1%) пропущенных значений. Более детальное исследование показывает, что все пропуски соответствуют отсутствию облицовки (перменная <sb>masvnrtype</sb> со значением NA). Соответственно, пропущенные значения площади для этих элементов заполняются нулевыми значениями.
                </p>
            <h2>
                <a
                    class='anchor'
                    id='model_base'>
                    </a>
                4. ML-модель №1 (базовая)
                </h2>
            <p>
                LightGBM используется в качестве базовой ML-модели как оптимальный с точки зрения эффективности и производительности.    
                <space></space>
                Катеориальные признаки кодируются с помощью <sb>OrdinalEncoder</sb> из библиотеки Feature-engine (категории заменяются целыми числами, порядок которых определяется согласно возрастанию среднего значения целевой переменной).
                <space></space>
                Подбор гиперпараметров проводится в два этапа. На первом этапе тестируется широкий диапазон значений и определяется базовые параметры. После этого проводится еще один поиск в окрестностях базовых параметров. В обоих случаях используется <sb>GridSearch</sb> от Scikit-learn с <sb>20-fold</sb> кросс-валидацией.
                <space></space>
                Чтобы предотвратить утчеку данных во время подбора гиперпараметров используется <sb>Pipeline</sb> от Scikit-learn.
                <space></space>
                Метрика качества модели - <sb>RMSE</sb> (среднеквадратичная ошибка).
                </p>
            <h4>
                <i>Результаты 1-го этапа подбора гиперпараметров</i>
                </h4> 
            <p>
                Лучшие результаты (RMSE в районе 0.130) показывают сложные модели с большим числом деревьев (> 25), увеличение числа листьев также положительно сказывается на эффективности модели. Оптимальная скорость обучения - 0.1. RMSE лучшей модели - 0.1282.
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/gridsearch1.png' style='width:200px'>
                <space></space>
                </p>
                <h4>
                    <i>Результаты 2-го этапа подбора гиперпараметров</i>
                    </h4> 
            <p>
                Ошибка уменьшилась незначительно - с 0.1282 до 0.1276.
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/gridsearch2.png'  style='width:200px'>
                <space></space>
                </p>
            <h4>
                <i>ML-модель №1 (базовая)</i>
                </h4>
            <p>
                Таким образом, базовая модель (RMSE: 0.1276) имеет следующую конфигурацию:
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/model_1.png'>
                <space></space>
                Это первый ориентир относительного того, насколько имеющиеся данные подходят для предсказания цены домов. Дополнительно базовая модель дает представление о наиболее значимых предикторах. Отметим, что 7 из 10 ключевых предикторов - это площадь того или иного помещения продаваемого дома. На общем фоне заметно выделяется суммарная площадь жилого пространства (<sb>grlivarea</sb>).
                <space></space>
                <img src='img/Section1-Overview-and-Base-model/feature_importance.png'>
                <space></space>
            <h2>
                <a
                    class='anchor'
                    id='eda'>
                    </a>
                5. Исследование данных
                </h2>
            <h4>
                <i>Матрица корреляций</i>
                </h4> 
            <p>
                Матрица корреляций деомнстрирует линейные взаимосвязи предикторов с целевой переменной. В топ-10 наиболее влияющих признаков в основном оценки площади и качества различных составляющих дома.
                <space></space>
                <img src='img/Section2-Explore-and-Clean/corr_matrix.png'  style='width:200px'>
                <space></space>
                Полный набор различных визуализаций представлен в соответсвующем блокноте <i>'Section2-Explore-and-Clean.ipynb'</i>. Ниже приведены несколько типовых графиков.
            </p>
            <h4>
                <i>Количественные признаки</i>
                </h4>
            <p>
                Ключевые инструменты для исследования количественных переменных - гистограммы и диаграммы рассеяния с добавлением линии регрессии.
                <space></space>
                <img src='img/Section2-Explore-and-Clean/quantitative.png' style='max-height:100%; max-width:100%'>
                <space></space>
                </p>
            <h4>
                <i>Категориальные признаки</i>
                </h4>
            <p>
                Ключевые инструменты для исследования катеориальных переменных - диаграмма рассеяния с лининей регрессии и точечная диаграмма.
                <space></space>
                <div 
                    class='pic-width-adjust'
                    style='background-image:url(img/Section2-Explore-and-Clean/ordinal.png);'>
                    </div>
                <space></space>
                <div 
                    class='pic-width-adjust'
                    style='background-image:url(img/Section2-Explore-and-Clean/categorical.png);'>
                    </div>
                <space></space>
                Примеры взаимосвязи некоторых категориальных переменных с ценой
                <space></space>
                <img src='img/Section2-Explore-and-Clean/housestyle.png'>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/kitchenqual_vs_price.png'>
                <space></space>
                </p>
            <h2>
                <a
                    class='anchor'
                    id='clear'>
                    </a>
                6. Очистка данных
                </h2>
            <h3>
                <a
                    class='anchor'
                    id='discrepancy'>
                    </a>
                6.1. Проверка данных на несоответствие
                </h3>
            <p>
                В этом пункте данные проверяются на логическое несоответствие. Например, суммарная площадь жилого пространства не может быть меньше суммарной площади всех комнат.
                <space></space>
                Всего проверено 33 предиктора, несоответствия найдены в двух переменных: <sb>masvnarea</sb> и <sb>garageyrblt</sb>.
                <space></space>
                <b> GARAGEYRBLT </b> (год постройки гаража) 
                <space></space>
                <i>Несоответствия:</i>
                </p>
                <ul>
                    <li>У 6 элементов год постройки гаража меньше года постройки дома. В некоторых случаях эта разница составляет несколько лет.</li>
                    </ul>
            <p>    
                <i>Возможная причина:</i>
                </p>
                <ul>
                    <li>Сооружение гаража может быть первым этапом строительства, но после его постройки возможна заморозка стройки, к примеру, из-за финансовых трудностей. Соответственно, сам дом будет достроен только через несколько лет. Такое несоответсвие допускается.</li>
                    </ul>
            <p>
                <i>Решение:</i>
                </p>
                <ul>
                    <li>Данные не исправляются.</li>
                    </ul>
            <p>
                <b> MASVNAREA </b> (Площадь облицовки фундамента)
                <space></space>
                <i>Несоответствия:</i>
                </p>
                 <ul>
                    <li>У 7 элементов площадь облицовки равна 0, при этом тип облицовки не NA, и наоборот (см. таблицу выше).</li>
                    </ul>
            <p>
                <i>Возможная причина:</i>
                </p>
                <ul>
                    <li><t>Ошибка ввода данных.</t></li>
                    </ul>
            <p>
                <i>Решение:</i>
                </p>
                <ul>
                    <li>Исправление вручную. Во время исправления ошибок в данных в первую очередь будем ориентироваться на тип облицовки (<sb>masvntype</sb>) и цену дома (<sb>price</sb>). На графике ниже видно, что наиболее дорогие - это дома с облицовкой из камня (Stone) и кирпича (BrkFace).</li>
                    </ul>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/masvnrtype_price.png'>
                <space></space>
            <p>
                В данных присутствует три типа противоречий. Каждое будет рассмотрено отдельно, и соответственно будут исследованы три группы элементов.
                </p>
            <h4>
                <i>Элементы с индексом 1230 и 733</i>
                </h4>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/1230_773.png'>
                <space></space>
            <p>
            <i>Противоречие:</i>
                </p>
            <ul>
                <li>Если <sb>masvntype</sb> равен NA, то <sb>masvnrarea</sb> должна быть равна 0.</li>
                </ul>
            <p>
                <i>Варианты исправления:</i>
                </p>
                <ul>
                    <li>Или <sb>masvntype</sb> присываивается какое-то значение, или <sb>masvnrarea</sb> приравнивается к 0.</li>
                    </ul>
            <p>
                <i>Решение:</i>
                </p>
                <ul>
                    <li><sb>masvnrarea</sb> приравнивается к 0.</li>
                    </ul>
            <p>
                <i>Аргументы:</i>
                </p>
                <ul>
                <li>Цены этих двух домов в районе 12.0 - это аргумет в пользу того, что облицовка отсутствует (средняя цена домов без облицовки - 11.9).</li>
                <li>Наименьшее значение переменной <sb>masvnarea</sb> - 10 (см. график ниже); в данном случае 1 - это вероятно ошибка ввода данных.</li>
                    </ul>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/masvnrarea_minimal.png'>
                <space></space>
            <h4>
                <i>Элементы с индексом 1241 и 688</i>
                </h4>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/688_1241.png'>
                <space></space>
            <p>
                <i>Противоречие:</i>
                </p>
                 <ul>
                    <li>Если <sb>masvnrarea</sb> равна нулю, то <sb>masvntype</sb> должен быть равен NA.</li>
                    </ul>
            <p>
                <i>Варианты исправления:</i>
                </p>
                 <ul>
                    <li>Или <sb>masvnrarea</sb> приравнивается какое-либо значение, или <sb>masvntype</sb> присываивается NA.</li>
                    </ul>
            <p>
                <i>Решение:</i>
                </p>
                <ul>
                    <li><sb>masvnrarea</sb> присваивается медианное значение элементов, имеющих соответствующее <sb>masvntype</sb>.</t></li>
                    </ul>
            <p>
                <i>Аргументы:</i>
                </p>
                <ul>
                    <li>Цена элементов с нулевой <sb>masvnrarea</sb> достаточно большая, говорит скорее в пользу наличия облицовки (т.е. в пользу того, что <sb>masvnrarea</sb> не равна нулю), поэтому <sb>masvnrarea</sb> заполняется медианным значением, соответствующим типу <sb>masvntype</sb>.</li>
                    </ul>
            <h4>
                <i>Элементы с индексом 624, 1300 и 1334</i>
                </h4>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/624_1300_1334.png'>
                <space></space>
            <p>
                <i>Противоречие:</i>
                </p>
                <ul>
                    <li>Если <sb>masvnrarea</sb> не равна нулю, то <sb>masvntype</sb> не должен быть NA.</li>
                    </ul>
            <p>
                <i>Варианты исправления:</i>
                </p>
                <ul>
                    <li>Или <sb>masvntype</sb> присываивается какое-то значение, или <sb>masvnrarea</sb> приравнивается к 0.</li>
                    </ul>
            <p>
                <i>Решение:</i>
                </p>
                <ul>
                    <li>Элементы 624 и 1334: <b>masvnrarea</b> приравнивается к 0;</li>
                    <li>Элемент 1300: <b>masvntype</b> присваивается значение Stone.</li>
                    </ul>
            <p>
                <i>Аргументы:</i>
                </p>
                <ul>
                    <li>Для всех трех элементов выбор сделан исходя из их цены - у дорогих домов чаще встречается <sb>masvntype</sb>, равный Stone, у недорогих - NA.</li>
                    </ul>
            <h4>
                <i>Исправленные данные</i>
                </h4>
                <space></space>
                <img src='img/Section2-Explore-and-Clean/masvnrarea_fixed.png'>
                <space></space>
            <h3>
                <a
                    class='anchor'
                    id='outliers'>
                    </a>
                6.2. Выявление и удаление выбросов
                </h3>
            <p>
                Два подозрительно высоких значения <sb>lotfrontage</sb>, обнаруженных при исследовании количественных признаков.
                <space></space>
                <img src='img/Section2-Explore-and-Clean/lotfrontage.png'>
                <space></space>
                Помимо этого потенциальные выбросы обнаружены в некотрых других переменных. К примеру, так выглядит диаграмма для переменной <sb>miscval</sb>:
                <space></space>
                <img src='img/Section2-Explore-and-Clean/miscval.png'>
                <space></space>
                Далее формируется список потенциальных выброосв и строятся диаграммы, на которых подозрительные элементы выделены цветом.
                <space></space>
                <img src='img/Section2-Explore-and-Clean/outliers_colored.png' style='width:200px'>
                <space></space>
                После визуальной оценки три элемента (индексы 523, 934 и 1298) признаются выбросами и удаляются из датасета.
                </p>
            <h2>
                <a
                    class='anchor'
                    id='model_inter1'>
                    </a>
                7. ML-модель №2 (после очистки данных)
                </h2>
            <p>
                Здесь применяется тот же двухэтапный подход, что и в пункте <i>"4. ML-модель №1 (базовая)"</i>. Лучшая модель демонстрирует RMSE на уровне 0.1233 (у базовой модели RMSE равна 0.1276). Конфигурация модели выглядит следующим образом:
                <space></space>
                <img src='img/Section2-Explore-and-Clean/model_2.png'>
                <space></space>
                По сравнению с базовой моделью число деревьев не изменилось, глубина дерева уменьшалсь почти в три раза, а количество листьев сократилось более чем в 10 раз. Модель стала проще и эффективнее. 
                <space></space>
                Очистка и исправление данных несколько сгладили значимость предикторов по сравнению с сырыми данными - по-прежнему лидирует предиктор, характеризующий суммарную жилую площадь <sb>grlivarea</sb>.
                <space></space>
                <img src='img/Section2-Explore-and-Clean/feature_importance.png'>
                <space></space>
                </p>
            <h2>
                <a
                    class='anchor'
                    id='feature_engineering'>
                    </a>
                8. Создание дополнительных предикторов 
                </h2>
            <p>
                На данном этапе предикторы разбиваются на <sb>группы</sb> и продолжается процесс исследования датасета. Изучается возможность создания дополнительных предикторов, которые гипотетически способны повысить эффективность модели.
                </p>
            <h3>
                <a
                    class='anchor'
                    id='square_feet'>
                    </a>
                8.1. Предикторы, характеризующие площадь
                </h3>
            <p>
                Исследованы 17 предикторов и сгенерирован 1 дополнительный - <sb>flsrmean</sb>.
                <space></space>
                Согласно матрице корреляций наиболее информативным предиктором в данной группе является суммарная площадь жилых помещений <sb>grlivarea</sb> с коэффициентом корреляции 0.72 (для очищенных данных). Этот показатель рассчитывается как сумма жилых площадей первого и второго этажа и по  сути эквивелентен их среднему арифметическому.
                <space></space>
                Гипотеза, лежащая в основе нового предиктора <sb>flrsfmean</sb>: для большинства жителей относительная ценнность первого этажа выше, чем второго. Это объясняется тем, что на втором этаже расположены преимещественно спальни, и люди в целом проводят больше времени на первом этаже. Чтобы учесть этот эффект, вместо среднего арифметического рассчитывается среднее взвешенное:
                </p>
                <space></space>
                $$ flrsfmean = \frac{first\_flrsf*weight_1 + second\_flrsf*weight_2}{weight_1 + weight_2} , $$
                <space></space>
                <p>
                    где \[ \mathit{weight_1} \] и \[ \mathit{weight_3} \] - это веса первого и второго этажа соответственно.
                    </p>
                <space></space>
                <p>
                    После нескольких тестов веса принимаются равными 1 и 0.7. Итоговая формула выглядит следующим образом:
                    </p>
                <space></space>
                $$ flrsfmean = \frac{first\_flrsf + second\_flrsf*0.7}{1.7} $$
                <space></space>
                <p>
                    Коэффициент корреляции нового предиктора <sb>flrsfmean</sb> равен 0.77 - это на 0.05 выше, чем у наиболее информатичного количественного предиктора <sb>grlivarea</sb>. В дополнение к этому, <sb>flrsfmean</sb> имеет мéньшую дисперсию.
                    </p>
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/flrsfmean.png' style='width:200px'>
                <space></space>
            <h3>
                <a
                    class='anchor'
                    id='rooms'>
                    </a>
                8.2. Предикторы, характеризующие помещения (жилые комнаты, ванные комнаты, кухни)
                </h3>
            <p>
                Исследованы 7 предикторов, сгенерировано 5 дополнительных - <sb>bedroomsize</sb>, <sb>kitchensize</sb>, <sb>bedroomfracrms</sb>, <sb>kitchenfracrms</sb>, <sb>bathsfracbedr</sb>. 
                <space></space>
                Наиболее информативный предиктор этой группы - <sb>fullbath</sb> (0.58), который показывает количество совмещенных ванных комнат. Новые предикторы можно разделить на три группы:
                </p>
                <ul>
                <li><sb>bedroomsize</sb>, <sb>kitchensize</sb>: относительный размер спален и кухонь от площади - рассчитывается как количество спален/кухонь, деленное на общую площадь жилых помещений (<sb>grlivarea</sb>);</li>
                <li><sb>bedroomfracrms</sb>, <sb>kitchenfracrms</sb>: относительный размер спален и кухонь от общего числа комнат - рассчитывается как количество спален/кухонь, деленное на общее число комнат (<sb>totrmsabvgrd</sb>);</li>
                <li><sb>bathsfracbedr</sb>: относительное число ванных комнат - рассчитывается как число совмещенных ванных комнат, деленное на число спален (<sb>bedroomabvgr</sb>).</li>
                    </ul>
            <p>
                Наиболее информативным из новых признаков является относительный размер кухни (<sb>kitchensize</sb>, коэффициент корреляции: -0.71).
                </p>
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/bedroom_kitchen.png' style='width:200px'>
                <space></space>
            <h3>
                <a
                    class='anchor'
                    id='dates'>
                    </a>
                8.3. Предикторы, характеризующие даты
                </h3>
            <p>
                Посчитаны и добавлены новые предикторы <sb>houseage</sb> (возраст дома на момент продажи) и <sb>modage</sb> (число лет с момента модернизации на момент продажи). Они не повышают информативность, но более удобны для интерпретации.
                </p>
            <h2>
                <a
                    class='anchor'
                    id='preprocessing'>
                    </a>
                9. Предварительная подготовка данных
                </h2>
             <p>
                <i>Первые пять эелементов данных до преобразования</i>
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/data_raw.png'>
                <space></space>
                 </p>
            <h4>
                <i>Количественные переменные</i>
                </h4>
            <p>
                Количественные предикторы логарифмируются (натуральный логарифм) с помощью <sb>LogCpTransformer</sb> из библиотеки Feature-engine. Предварительно к каждому значению добавляетя константа C, равная 1. В название всех логарифимрованных предикторов добавляется приставка <sb>lg_</sb>.
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/data_log.png'>
                <space></space>
                Матрица корреляций после добавления новых предикторов приведена ниже. Единственный признак, отрицательно коррелирующий с целевой перменной - относительный размер кухни <sb>kitchensize</sb>.
                <space></space>    
                <img src='img/Section3-Feature-selection-and-Preprocessing/corr_matrix_new.png'  style='width:200px'>
                <space></space>
                </p>
            <h4>
                <i>Категориальные переменные</i>
                </h4>
            <p>
                Для преобразования категориальных переменных используется <sb>OrdinalEncoder</sb> из библиотеки Feature-engine. Он присваивает переменным порядковые целочисленные значения (0, 1, 2, 3, и т.д.). Порядок определяется средним значением целевой переменной (0 - для элементов с наименьшим средним и далее по возрастанию).
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/data_encoded.png'>
                <space></space>
                </p>
            <h4>
                <i>Стандартизация</i>
                </h4>
            <p>
                После этого данные стандартизируются с помощью <sb>StandardScaler</sb> из библиотеки Scikit-learn.
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/data_scaled.png'>
                <space></space>
                </p>
            <h2>
                <a
                    class='anchor'
                    id='model_inter2'>
                    </a>
                10. ML-модель №3 (после создания новых признаков)
                </h2>
            <p>
                Добавление новых предикторов практически не оказало влияния на эффективность модели (RMSE уменьшилась с 0.1233 до 0.1230). Число деревьев и листьев не изменились, а глубина деревьев увеличилась в два раза.
                <space></space>
                <img src='img/Section3-Feature-selection-and-Preprocessing/model_3.png'>
                <space></space>
                 Теперь считает самым информативным предиктором является вновь созданный <sb>flrsfmean</sb> (это соответствует его высокому коэффициенту корреляции). В топ-10 наиболее значимых признаков также попал новый предиктор <sb>bedroomsize</sb>.
                <space></space>    
                <img src='img/Section3-Feature-selection-and-Preprocessing/feature_importance.png'>
                <space></space>
                </p>
            <h2>
                <a
                    class='anchor'
                    id='model_linear'>
                    </a>
                11. Линейные модели и регрессионный анализ
                </h2>
            <h3>
                <a
                    class='anchor'
                    id='elastic_net'>
                    </a>
                11.1. Эластичная сеть
                </h3>
            <p>
                К текущему моменту мы получили представление о наиболее значимых предикторах благодаря встроенному функционалу LightGBM и матрице корреляций. В данном пункте мы обучим эластичную сеть, оценим влияние регуляризации и посмотрим, какие предикторы выбирает сеть в зависимости от величины параметра регуляризации.
                <space></space>
                Отобразим пространство значений гиперпараметра $\alpha$
                <space></space>
                <img src='img/Section4-Linear-models-research/alphas_logspace.png'>
                <space></space>
                Выполним поиск по сетке гиперпараметров <sb>GridSearch</sb> и отобразим результаты на графике. В рамках исследуемых данных лучшую предиктивную эффективность показывает минимальная регуляризация.
                <space></space>
                <img src='img/Section4-Linear-models-research/regularization.png'>
                <space></space>
                Теперь посмотрим, какие предикторы выбирает эластичная сеть в зависимости от величины $\alpha$, и какое влияние это оказывает на эффективность модели.
                <space></space>
                <img src='img/Section4-Linear-models-research/features_plot.png'>
                <space></space>
                Чем ниже параметр регуляризации, тем выше эффективность прогнозирования. Минимальное число предикторов эластичной сети - 3, это уже известные <sb>lg_flrsfmean</sb>, <sb>neighborhood</sb>, <sb>overallqual</sb>. Ошибка резко уменьшается при увеличинии количества предикторов с 3 до 8, заметно снижается при изменении их с 8 до 33, и плавно снижается после увеличения до 88. С точки зрения баланса точности прогнозов и интерпретируемости, оптимальное количество предикторов находится где-то между 8 и 33. Необходимо также учитывать тот факт, что после отметки в 14 признаков показатель мультиколлинеарности <sb>VIF</sb> увеличивается многократно и выходит за допустимые рамки (см. таблицу ниже). 
                <space></space>
                <i>Изменение набора предикторов в зависмости от величины параметра регуляризации $\alpha$</i>
                <space></space>
                <img src='img/Section4-Linear-models-research/features_by_alpha.png'>
                <space></space>
                Отметив эти особенности, можно переходить к следующему пункту - построению линейной регрессии, которая станет ядром HousePricePredictor.
                </p>
            <h3>
                <a
                    class='anchor'
                    id='linear_regression'>
                    </a>
                11.2. Линейная регрессия
                </h3>
            <p>
                Предикторы должны соответствовать следующим требованиям:
                </p>
            <ul>
                <li>Они удобны в интерпретации и понятны любому пользователю;</li>
                <li>Они входят в состав наиболее информативных предикторов эластичной сети;</li>
                <li>Они статистически значимы на уровне $\alpha$, равном 0.05;</li>
                <li>Включение дополнительного признака в модель увеличивает скорректированный коэффициент детерминации $R^{2}_{adj}$.</li>
                </ul>
            <p>
                В итоге выбраны следующие предикторы:
                </p>
            <ul>
                <li><sb>lg_flrsfmean</sb> - средневзвешенная площадь этажей;</li>
                <li><sb>lg_lotarea</sb> - площадь придомовой территории;</li>
                <li><sb>overallqual</sb> - оценка качества дома;</li>
                <li><sb>kitchenqual</sb> - оценка качества кухни;</li>
                <li><sb>exterqual</sb> - оценка качества материала фасада;</li>
                <li><sb>bsmtqual</sb> - оценка качества подавала;</li>
                <li><sb>overallcond</sb> - оценка состояния дома;</li>
                <li><sb>houseage</sb> - возраст дома на момент продажи;</li>
                <li><sb>garagecars</sb> - количество машиномест в гараже.</li>
                    </ul>
            <p>
                Для надежной оценки доверительных интервалов коэффициентов используется метод HC3, устойчивый к гетероскедастичности.
                <space></space>
                <i>Коэффициенты регрессии</i>
                <space></space>
                <img src='img/Section4-Linear-models-research/coefficients_table_charachteristics.png' style='width:200px'>
                <space></space>
                </p>
            <h4>
                <i>Оценка качества регрессии</i>
                </h4>
            <p>
                <i>Визуальная оценка</i>
                </p>
                <space></space>
                <div
                    class='pic-width-adjust'
                    style='background-image: url(img/Section4-Linear-models-research/regression_diagnostics.png);'>
                    </div>
            <p>
                <space></space>
                <i>Результаты статистических тестов и характеристики регрессии</i>
                <space></space>
                <img src='img/Section4-Linear-models-research/regression_tests.png'>
                <space></space>
                Скорректированный коэффициент детерминации $R^{2}_{adj}$ значим и равен 0.882.
                <space></space>
                Согласно статистическим тестам у нас недостаточно оснований утверждать, что в регрессию включены все предикторы, оказывающие влияние на целевую переменную, и отсутствуют нелинейные взаимосвязи (тест Ramsey, нелинейность остатков). Автокорреляция остатков и мультиколлинеарность не выявлены. Гетероскедастичность остатков нивелируется поправкой HC3. 
                <space></space>
                Ни один тест не дает оснований считать, что остатки распределены нормально. Попробуем удалить крайние элементы ($1.5*IQR$), повторить тесты на нормальность и эксцесс, а также построим график, на котором нормальное распределение с параметрами остатков ($mean$ 0, $std.$ ~0.1243) сравнивается с остатками до и после удаления выбросов. 
                <space></space>
                <i>Тесты на нормальность распределения остатков без выборосов</i>
                <space></space>
                <img src='img/Section4-Linear-models-research/residuals_trimmed_tests.png'>
                <space></space>
                <i>Коэффициент эксцесса</i>
                <space></space>
                <img src='img/Section4-Linear-models-research/residuals_kurtosis.png'>
                <space></space>
                <i>Визуализация формы распределения остаков и сравнение с нормальным распределением</i>
                <space></space>
                <img src='img/Section4-Linear-models-research/residuals_distributions_comparing.png' style='width:200px'>
                <space></space>
                Теперь остатки распределены нормально, и в целом форма остатков без удаления выбросов отличается от формы нормального распределения с заданными параметрами только наличием хвостов, что сказывается и на величине эксцесса. Возможно, структура цены нескольких элементов выбивается из общей картины. Можно предположить сознательное снижение/завышение цены при продаже дома.
                </p>
            <h4>
                <i>Интерпретация коэффициентов</i>
                </h4>  
            <p>
                Все оценки действиетельны при прочих равных:
                </p>
                <ul>
                    <li><sb>lg_flrsfmean</sb> - при увеличении средневзвешенной площади этажей на 1 %, цена увеличивается на 0.144 %;</li>
                    <li><sb>overallqual</sb> - при увеличении качества материалов на 1 пункт, цена увеличивается на 8.8 %;</li>
                    <li><sb>houseage</sb> - при увеличении возраста дома на 1 год цена уменьшается на 8.5 %;</li>
                    <li><sb>lg_lotarea</sb> - при увеличении площади придомовой территории на 1 %, цена увеличивается на 0.0612 %;</li>
                    <li><sb>overallcond</sb> - при увеличении оценки состояния дома на 1 пункт, цена увеличивается на 0.0558 %;</li>
                    <li><sb>bsmtqual</sb> - при увеличении оценки качества подавала на 1 пункт, цена увеличивается на 0.0459 %;</li>
                    <li><sb>garagecars</sb> - при увеличении количество машиномест на 1 единицу, цена увеличивается на 0.0316 %;</li>
                    <li><sb>kitchenqual</sb> - при увеличении оценки качества кухни на 1 пункт, цена увеличивается на 0.0266 %;</li>
                    <li><sb>exterqual</sb> - при увеличении оценки качества материала фасада на 1 пункт, цена увеличивается на 0.0203 %;</li>
                    </ul>
            <h4>
                <i>Вывод</i>
                </h4>
            <p>
                Линейная регрессия объясняет 88% изменчивости целевой переменной, что является хорошим результатом. Используемые предикторы значимы, понятны и легко интерпретируемы. Тем не менее, модель не в полной мере соответствует имеющимся данным. Это объяснимо, если учесть тот факт, что не были использованы такие важные признаки, как <sb>neighborhood</sb> или <sb>functional</sb>.
                </p>
            <h3>
                <a
                    class='anchor'
                    id='simulations'>
                    </a>
                11.3. Результаты линейных моделей на сэмплированных данных
                </h3>
             <p>
                Сгенерированные с помощью симуляций подвыборки данных не содержат дополнительной информации. Тем не менее, если предположить, что исходный датасет достаточно точно отражают генеральную совокупность, результаты симуляций могут быть полезны для оценки дисперсии прогнозов.
                <space></space>
                Симуляции проводились на 1000 сэмплах с повторением (число элементов в сэмпле и исходных данных одинаково). 
Ошибки RMSE составили 0.1352 для обученной в п.12.2. линейной регрессии и 0.1347 для эластичной сети с гиперпараметрами {'alpha': 0.0067, 'l1_ratio': 0.1}.
                <space></space>    
                <img src='img/Section4-Linear-models-research/simulations.png'>
                <space></space>
                Прогнозы отдельно взятых элементов практически не различаются (что показывают предиктивные интервалы). Однако на длинной дистанции (для бóльшего числа элементов) в среднем эффективность эластичной сети выше, поэтому она будет включена в итоговое сравнение. 
                </p>
            <h2>
                <a
                    class='anchor'
                    id='residuals'>
                    </a>
                12. Выбор алгоритма машинного обучения для прогнозирования остатков линейной регрессии
                </h2>
            <p>
                В этом разделе выбирается алгоритм для прогнозирования остатков линейной регрессии (12.1., 12.2., 12.3.). После этого создается класс <sb>HousePricePredictor</sb>, который объединяет в себе линейную регрессию на первом уровне и оценщик остатков на втором уровне (12.4.).
            <h3>
                <a
                    class='anchor'
                    id='ml_simple'>
                    </a>
                12.1. Опорные вектора, бустинги, бэггинг, случайный лес, K-ближайших соседей
                </h3>
             <p>
                На первом этапе обучаются следюущие ML-модели:
                 </p>
                <ul>
                    <li><sb>KNN</sb> - K-Nearest Neighbors;</li>
                    <li><sb>SVR</sb> - Support Vector Machine;</li>
                    <li><sb>RF</sb> - Random Forest;</li>
                    <li><sb>Bagging</sb> - Bagging Regressor;</li>
                    <li><sb>LGBoost</sb> - Light Gradient Boosting;</li>
                    <li><sb>HGBoost</sb> - Histogram-based Gradient Boosting;</li>
                    <li><sb>XGBoost</sb> - Extreme Gradient Boosting.</li>
                    </ul>
            <p>
                В качестве предикторов используются все сгенерированные в предыдущих пунктах признаки, а также прогнозы линейной регрессии (предиктор <sb>price_pred_lr</sb>).
                <space></space>
                Алгоритмы <sb>Random Forest</sb>, <sb>Bagging</sb>, <sb>LGBoost</sb>, <sb>XGBoost</sb> имеют встроенный функционал отбора наиболее информативных признаков. Отобразим топ-10 наиболее важных признаков для каждой из моделей.
                <space></space>       
                <img src='img/Section5-Residuals-prediction/features_importance_four_models.png'>
                <space></space>
                Наиболе значимые признаки здесь:
                <space></space>
            <ul>
                <li><sb>neighborhood</sb> - район расположения дома, он вошел в топ-3 ключевых признаков каждой из моделей;</li>
                <li><sb>lg_bsmtfinsf_first</sb> - площадь подвала 1 типа, 3 модели поместили его в топ-3 ключевых признаков;</li>
                <li><sb>price_pred_lr</sb> - также в топ-3 у трех моделей, но менее значим, чем <sb>lg_bsmtfinsf_first.</sb></li>
                </ul>
            <p>
                <i>Число попаданий в топ-10 ключевых признаков</i>
                </p>
                <space></space>
                <img src='img/Section5-Residuals-prediction/features_importance.png'>
                <space></space>
            <p>
                Результаты на 20-фолдовой кросс-валидации выглядят следующим образом (опорные вектора продемонстрировали неожиданно высокий результат).
                <space></space>
                <img src='img/Section5-Residuals-prediction/models_residuals.png'>
                <space></space>
                </p>
            <h3>
                <a
                    class='anchor'
                    id='vote_stack'>
                    </a>
                12.2. Добавление оценщиков верхнего уровня - Voting и Stacking
                </h3>
            <p>
                На втором этапе оценивается эффект от поочередного включения в модель оценщиков верхнего уровня.
                <space></space>
                <b>VoteRegressor</b>
                <space></space>
                VoteRegressor формирует свой прогноз, усредняя прогнозы моделей нижнего уровня. Средняя RMSE на 20-фолдовой проверке составила 0.1101.
                <space></space>
                <b>StackingRegressor</b> 
                <space></space>
                В схеме StackingRegressor на верхний уровень добавляется агрегирующий оценщик (в данном случае это <sb>Ridge Regression</sb>), который выдает прогнозы, используя в качестве предикторов прогнозы моделей нижнего уровня. Средняя RMSE на 20-фолдовой проверке: 0.1084.
                <space></space>
                Оба метаоценщика демонстрируют лучшие результаты по сравнению с моделями по-отдельности. Визуализируем результаты их перекрестной проверки.
                <space></space>
                <i>Сравнение Voting и Stacking (20-fold cross-validation)</i>
                <space></space>
                <img src='img/Section5-Residuals-prediction/vote_stack_compare.png'>
                <space></space>
                Прогнозы <sb>Stacking</sb> точнее в 14 фолдах из 20.
            <h3>
                <a
                    class='anchor'
                    id='residuals_final'>
                    </a>
                12.3. Сравнение всех ML-алгоритмов и выбор оценщика остатков регрессии
                </h3>
            <p>
                Суммарные результаты всех обученных в этом разделе моделей выглядят следующим образом
                <space></space>
                <img src='img/Section5-Residuals-prediction/models_residuals_summary.png'>
                <space></space>
                <i>Визуализация результатов</i>
                <space></space>
                <img src='img/Section5-Residuals-prediction/final_plot.png'>
                <space></space>
                В качестве оценщика остатков будет использоваться <sb>Stacking</sb>.
                <space></space>
                <i>Конфигурация StackingRegressor</i>
                <space></space>
                <!-- <div
                    class='pic-width-adjust'
                    style='background-image:url(img/Section5-Residuals-prediction/stacking_cofiguration.png)'>
                    </div> -->
                </p>
            <h3>
                <a
                    class='anchor'
                    id='hpp'>
                    </a>
                12.4. Финальный оценщик HousePricePredictor
                </h3>
            <p>
                Класс <sb>HousePricePredictor</sb> объединяет линейную регрессию и <sb>Stacking</sb> в единую модель. Линейная регрессия генерирует основные прогнозы цены, а <sb>Stacking</sb> используется для коррекции и повышения точности прогнозов.
                <space></space>
                <img src='img/Section5-Residuals-prediction/hpp.png'>
                <space></space>
                <sb>HousePricePredictor</sb> поддерживает Scickit-learn-совместимые методы <sb>fit</sb> и <sb>predict</sb>, а также ряд других методов (код доступен в файле <i>tools.py</i>)
                <space></space>
                <img src='img/Section5-Residuals-prediction/fit.png'>
                <space></space>
                <img src='img/Section5-Residuals-prediction/predict.png'>
                <space></space>
                Средняя RMSE на 20-фолдовой проверке: 0.1102 +/- 0.009.
                </p>
            <h2>
                <a
                    class='anchor'
                    id='ml_models'>
                    </a>
                13. Обучение независимых ML-моделей
                </h2>
            <p>
                Аналогично выбору прогозирования остатков в первую очередь обучаются популярные ML-алгоритмы, а после этого исследуется влияние оценщиков верхнего уровня <sb>Voting</sb> и <sb>Stacking</sb>.
                <space></space>
                Катеориальные признаки кодируются с помощью <sb>OrdinalEncoder</sb> из библиотеки Feature-engine (категории заменяются целыми числами, порядок которых определяется согласно возрастанию среднего значения целевой переменной).
                <space></space>
                Подбор гиперпаратмеров осуществляется с помощью решетчатого поиска <sb>GridSearch</sb> от Scikit-learn с <sb>20-fold</sb> кросс-валиадацией. Чтобы предотвратить утчеку данных во время подбора гиперпараметров используется <sb>Pipeline</sb> от Scikit-learn.
                <space></space>
                Метрика качества модели - <sb>RMSE</sb> (среднеквадратичная ошибка).
                <space></space>
                Для оценки вклада предикторов, добавленных в пункте <i>'8. Создание дополнительных предикторов'</i>, модели обучаются с добавлением и без добавления новых признаков.
                <space></space>
                <i>Визуализация результатов</i>
                <space></space>
                <img src='img/Section7-ML-models/ml_models_plot.png'>
                <space></space>
                <i>Средние значения RMSE (20-fold) всех моделей</i>
                <space></space>
                <img src='img/Section7-ML-models/ml_models_table.png'>
                <space></space>
                Ожидаемо, наиболее эффективным алгоритмом оказался <sb>Stacking</sb>. Его конфигурация идентична модели, выбранной для прогнозирования остатков регрессии.
                <space></space>
                <!-- <div
                    class='pic-width-adjust'
                    style='background-image:url(img/Section7-ML-models/stacking_cofiguration.png)'>
                    </div> -->
                </p>
            <h2>
                <a
                    class='anchor'
                    id='final_results'>
                    </a>
                14. Подведение итогов
                </h2>
            <h3>
                <a
                    class='anchor'
                    id='all_models'>
                    </a>
                14.1. Сравнение всех моделей
                </h3>
            <p>
                В данном пункте оценщик HousePricePredictor сравнивается с моделями из предыдущих пунктов. Три базовых модели LightGBM обучены на разных этапах подготовки данных (удаление пропусков, очистка, генерация признаков). Для обучения регрессионной модели (эластичная сеть) используются те же предикторы, которые были использованы для линейной регрессии в HousePricePredictor. Две ML-модели представляют лучшую линейную (SVM) ML-модель ансамблевый алгоритм (Stacking).
                </p>
            <ul>
                <li><sb>Base</sb> - базовая модель (LightGBM) после удаления пропусков;</li>
                <li><sb>Base (Clean)</sb> - промежуточная модель (LightGBM) после очистки данных;</li>
                <li><sb>Base (FE)</sb> - промежуточная модель (LightGBM) после генерации новых признаков;</li>
                <li><sb>Elastic Net</sb> - лучшая регрессионная модель (эластичная сеть);</li>
                <li><sb>SVM</sb> - лучшая линейная модель (SVM);</li>
                <li><sb>Stacking</sb> - лучшая ML-модель (Stacking);</li>
                <li><sb>HPP</sb> - модель, полученная в результате комбинации линейной регрессии и стэкинга (HousePricePredictor).</li>
                </ul>
            <p>
                <i>Результаты на тестовом датасете</i>
                <space></space>
                <img src='img/Section8-Final-predictions/final_bar.png'>
                <space></space>
                <i>Сводный график и таблица с результатами</i>
                <space></space>
                <img src='img/Section8-Final-predictions/final_plot.png'>
                <space></space>
                <img src='img/Section8-Final-predictions/final_table_T.png'>
                <space></space>
                Ряд комментариев относительно результатов:
                </p>
            <ul>
                <li>Комбинирование линейной регрессии и алгоритмов машинного обучения превзошли ожидаемые результаты. Итоговый оценщик не только не уступает лучшей модели, но и превосходит её в точности;</li>
                <li>Очистка и исправление данных увеличили прогнозную точность базовой модели и уменьшили доверительный интревал среднего ошибок;</li>
                <li>Генерация ноывых признаков сказалась конфигурации базовой модели, но не на на точности прогнозов (при этом нельзя утверждать, что новые признаки бесполезны - они использовались другими моделями, в частности линейной регрессией HousePricePredictor);</li>
                <li>Эластичная сеть ожидаемо продемонстрировала худшие результаты (Во многом это связано с ограниченным числом предикторов в этой модели);</li>
                <li>Опорные вектора (SVM) неожиданно продемонстрировали сравнительно очень хорошие результаты. По эффективность этот алгоритм уступил только стэкингу (Stacking). Можно предположить, что во многом это обусловлвено общей линейностью в данных;</li>
                <li>Stacking и HPP показывают хорошую кучность (это видно на нижнем графике swarmplot).</li>
                </ul>
            <p>
                Дополнительно визуализируем ошибку всех 20 фолдов кросс-валидации HousePricePredictor и Stacking
                <space></space>
                <img src='img/Section8-Final-predictions/stack_hpp_compare.png'>
                </p>
            <h3>
                <a
                    class='anchor'
                    id='predicted_actual'>
                    </a>
                14.2. Актуальные и спрогнозированные цены
                </h3>
            <p>
                <i>Сопоставление актуальных и спрогнозированных HousePricePredictor цен (элементы по оси Х отсортированы в сторону уеличения суммарной жилой площади)</i>
                <space></space>
                <img src='img/Section8-Final-predictions/predicted_actual_residuals.png'>
                <space></space>
                В первой трети графика видно, что HousePricePredictor излишне оптимитичен и часто переоценивает дома. Во второй трети этот баланс выравнивается, но в третьей части графика появляется снова. По-видимому, выбранная нами архитектура построения модели хорошо подходит для домов среднего размера, но для больших и маленьких домов модель должна быть пересмотрена, что в теории приведет к увеличению воможностей для эффективного прогноза цены.
                </p>
            <h3>
                <a
                    class='anchor'
                    id='hpp_residuals'>
                    </a>
                14.3. Остатки HPP
                </h3>
            <p>
                В данном пункте посмотрим, как ведут себя остатки. В этом проекте немного нарушена последовательность действий. Здесь исследуются результаты прогнозирования на test-датасете. На практике исследование остатков, сравнение актуальной и спрогнозированоой цен необходимо делать на train-датасете, чтобы скорректировать модель и внести концепутаьные исправления перед финальным тестом. Тем не менее, здесь этот требование нарушено, так как проект демонстрационный и не будет развиваться в дальнейшем. 
                <space></space>
                <i>Распределение остатков HousePricePredictor</i>
                <space></space>
                <img src='img/Section8-Final-predictions/hpp_residuals.png'>
                <space></space>
                Ни один тест не дает основания предполагать, что остатки распределены нормально. Попробуем удалить экстремальные значения ошибок (шесть эелментов с остатками более 50000$, см. таблицу ниже).
                <space></space>
                <img src='img/Section8-Final-predictions/residuals_been_cutted.png'>
                <space></space>
                После этого три из пяти тестов указывают на нормальность распределения.
                <space></space>
                <img src='img/Section8-Final-predictions/residuals_cutted_tests.png'>
                <space></space>
                <i>Сравнение формы распределения остатков (до и после удаления выбросов) с формой нормального распределения</i>
                <space></space>
                <img src='img/Section8-Final-predictions/residuals_distributions_compare.png'>
                <space></space>
                Это подтверждает предположение о наличии несколько элементов, которые не вписыааются в общий контекст. Возможно по причине ошибки ввода данных или мошенничестве при заключении сделки о продаже дома.
                <space></space>
                Далее отобразим взаимосвязь актуальных и спрогнозированных цен. Область между 100000 и 200000 имеет низкую дисперсию, в то время как у более дорогих домов разброс сильнее. 
                <space></space>
                <img src='img/Section8-Final-predictions/actual_predicted.png'>
                <space></space>
                Попробуем определить предиктор, который наилучшим образом отражает эту закономерность. После ряда тестов выяснилось, что таким предиктором является суммарная площадь жилых помещений <sb>grlivarea</sb> (что логично, так как этот признак наиболее сильно коррелирует с ценой).
                <space></space>
                Рассчитаем <sb>абсолютные</sb> и <sb>относительные ошибки</sb> и отобразим их взаимосвязь с <sb>grlivarea</sb>.
            <h4>
                <i>Абсолютные ошибки</i>
                </h4>
            <p>
                Для домов с суммарной жилой площадью менее 1900 футов RMSE равна 0.084705. С увеличением площади дома значительно возрастает и дисперсия прогнозов - RMSE равна 0.132874. 
                <space></space>
                <img src='img/Section8-Final-predictions/grlivarea_residuals.png'>
                <space></space>
                </p>
            <h4>
                <i>Относительные ошибки (предсказанная цена относительно актуальной цены)</i>
                </h4>
            <p>
                С другой стороны, если взглянуть на относительные ошибки, то зависимость от величины жилого пространства менее очевидна. Можно увидеть некоторые паттерны, если дополнительно построить линии локальной регрессии (LOWESS). Форма остатков в левой части графика напоминает форму вогнутой пораболы с вершиной в районе площади, равной 1250. В правой же части графика явно-выраженная закономерность отсутствует (что может быть следствием недостаточного числа элементов).
                <space></space>
                <img src='img/Section8-Final-predictions/grlivarea_residuals_frac.png'>
                </p>
            <h3>
                <a
                    class='anchor'
                    id='further'>
                    </a>
                14.4. Дальнейшие шаги
                </h3>
            <h4>
                <i>Предложение 1</i>
                </h4>
            <p>
                 В пункте <i>"14.3. Остатки HPP"</i> показано, что дисперсия остатков увеличивается с увеличением площади дома. Чем больше величина ошибок в абсолютном выражении, тем выше потери от неверно предсказанных цен. Поэтому можно разбить исходные данные на две группы - услвно большие дома с суммарной площадью выше 1900 квадратных футов и маленькие дома с площадью ниже 1900 - и исследовать их отдельно друг от друга, чтобы построить две разные модели.
                </p>
            <h4>
                <i>Предложение 2</i>
                </h4>
            <p>
                Используя схему <sb>Stacking</sb> построить ансамбль, в котором алгоритмы нижнего уровня обучаются не на полных данных, а на группах, сформированных в пункте <i>"8. Создание дополнительных предикторов"</i> - площадь, география, качество материалов и т.д.
                </p>
            <h4>
                <i>Предложение 3</i>
                </h4> 
            <p>
                Предиктор <sb>neighborhood</sb> имеет высокий коэффициент корреляции (0.81), и высоко котируется большинством моделей машинного обучения. При этом он не использовался для построения линейной регрессии, чтобы не усложнять её интерпретацию. В перспективе можно более детально изучить географию местности и её взаимосвязь с целевой переменной. Возможно, объединить некоторые районы в один предиктор. 
            <space></space>
            Еще один предиктор, на который стоит обратить внимание - это <sb>functional</sb>.
                </p>
            
            
            </td>
        <td style='width:55%;border:none'></td>
        </tr>










    


    <tr>
        <td style='width:20%;border:none'>
            1
            </td>
        <td style='width:65%;border-right:1px solid #CFD7DE;border-left:1px solid #BBBBBB'>
            3
            </td>
        <td style='width:55%;border:none'>
            4
            </td>
        </tr>


</table>

    
</body>


